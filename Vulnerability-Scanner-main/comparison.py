import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Set style for better visualization
plt.style.use('ggplot')
sns.set(font_scale=1.2)
colors = sns.color_palette("viridis", 3)

def load_model_results():
    """
    Get model results - either from saved models or use synthetic results
    if models are not available.
    """
    # Check if models are available
    if os.path.exists("models/rf_model.pkl") and os.path.exists("models/scaler.pkl"):
        # Load actual results from saved models
        print("Loading results from saved models...")
        
        try:
            # This would be your actual test data in a real scenario
            X_test, y_test = load_test_data()
            scaler = joblib.load("models/scaler.pkl")
            X_test_scaled = scaler.transform(X_test)
            
            results = {}
            model_names = {"rf": "Random Forest", "svm": "SVM", "nn": "Neural Network"}
            
            for name_short, name_long in model_names.items():
                model = joblib.load(f"models/{name_short}_model.pkl")
                y_pred = model.predict(X_test_scaled)
                
                # Calculate metrics
                mse = mean_squared_error(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                
                results[name_long] = {"MSE": mse, "MAE": mae, "R²": r2}
            
            return results
        except Exception as e:
            print(f"Error loading models: {e}")
            print("Using synthetic results for visualization...")
    
    # If models aren't available, use synthetic results for demonstration
    print("Using synthetic results for visualization...")
    return {
        "Random Forest": {"MSE": 0.35, "MAE": 0.45, "R²": 0.92},
        "SVM": {"MSE": 0.58, "MAE": 0.62, "R²": 0.85},
        "Neural Network": {"MSE": 0.42, "MAE": 0.51, "R²": 0.89}
    }

def load_test_data():
    """
    Generate synthetic test data.
    In a real scenario, you would load your actual test dataset.
    """
    np.random.seed(43)  # Different seed from training
    n_samples = 200
    
    # Feature matrix X with 20 features
    X = np.zeros((n_samples, 20))
    
    # Generate features similar to the training data
    tool_types = np.random.randint(0, 4, n_samples)
    for i in range(n_samples):
        X[i, tool_types[i]] = 1
    
    for i in range(4, 10):
        X[:, i] = np.random.binomial(1, 0.3, n_samples)
    
    X[:, 10] = np.random.uniform(0.5, 5, n_samples)
    
    for i in range(11, 15):
        X[:, i] = np.random.poisson(2, n_samples)
    
    for i in range(15, 20):
        X[:, i] = np.random.binomial(1, 0.25, n_samples)
    
    # Generate target variable (CVSS scores)
    y = np.zeros(n_samples)
    y += X[:, 4] * 3 + X[:, 8] * 4  # SQL injection & RCE weight
    y += X[:, 5] * 2 + X[:, 9] * 1.5  # XSS & CSRF weight
    y += X[:, 15] * 2 + X[:, 17] * 2.5  # Auth issues & Data leakage
    y += np.random.normal(0, 1, n_samples)
    y = np.clip(y, 0, 10)
    
    return X, y

def create_bar_graph_comparison(results):
    """
    Create bar graphs to compare model performance metrics.
    """
    # Convert results to DataFrame suitable for plotting
    models = []
    metrics = []
    values = []
    
    for model_name, model_metrics in results.items():
        for metric_name, metric_value in model_metrics.items():
            models.append(model_name)
            metrics.append(metric_name)
            if metric_name == "R²":
                # Convert R² to percentage
                values.append(metric_value * 100)
            else:
                # Convert error metrics to percentage of CVSS scale (0-10)
                values.append((metric_value / 10) * 100)
    
    df = pd.DataFrame({
        'Model': models,
        'Metric': metrics,
        'Value (%)': values
    })
    
    # Create subplots for each metric
    fig, axes = plt.subplots(1, 3, figsize=(18, 8))
    fig.suptitle('CVSS Prediction Model Performance Comparison', fontsize=20)
    
    # Customize what better means for each metric
    metrics_info = {
        "MSE": {"title": "Mean Squared Error", "ylabel": "Error (%)", "better": "lower"},
        "MAE": {"title": "Mean Absolute Error", "ylabel": "Error (%)", "better": "lower"},
        "R²": {"title": "R² Score", "ylabel": "Accuracy (%)", "better": "higher"}
    }
    
    # Plot each metric in its own subplot
    for i, metric in enumerate(["MSE", "MAE", "R²"]):
        metric_df = df[df['Metric'] == metric].copy()
        
        # Sort bars based on performance
        if metrics_info[metric]["better"] == "lower":
            metric_df = metric_df.sort_values('Value (%)')
        else:
            metric_df = metric_df.sort_values('Value (%)', ascending=False)
        
        ax = axes[i]
        bars = sns.barplot(
            x='Model', 
            y='Value (%)', 
            data=metric_df, 
            palette=colors,
            ax=ax
        )
        
        # Add title and labels
        ax.set_title(metrics_info[metric]["title"], fontsize=16)
        ax.set_ylabel(metrics_info[metric]["ylabel"], fontsize=14)
        ax.set_xlabel('')
        
        # Add text labels on bars
        for bar in bars.patches:
            height = bar.get_height()
            ax.text(
                bar.get_x() + bar.get_width()/2.,
                height + 0.5,
                f'{height:.1f}%',
                ha='center', 
                fontsize=12
            )
        
        # Add "better" indicator
        if metrics_info[metric]["better"] == "lower":
            arrow_y = metric_df['Value (%)'].min() / 2
            ax.annotate('Lower is better', xy=(0.5, arrow_y), 
                        xytext=(0.5, arrow_y - 10),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        ha='center', fontsize=12)
        else:
            arrow_y = metric_df['Value (%)'].max() / 2
            ax.annotate('Higher is better', xy=(0.5, arrow_y), 
                        xytext=(0.5, arrow_y + 10),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        ha='center', fontsize=12)
            
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig('model_comparison_bar_graphs.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Create a single grouped bar chart for direct comparison
    plt.figure(figsize=(12, 8))
    
    # For grouped bar chart, reshape the data
    grouped_bar = sns.barplot(
        x='Model', 
        y='Value (%)', 
        hue='Metric',
        data=df,
        palette=sns.color_palette("viridis", 3)
    )
    
    plt.title('CVSS Model Performance - All Metrics Comparison', fontsize=18)
    plt.ylabel('Percentage (%)', fontsize=14)
    plt.xlabel('Model', fontsize=14)
    plt.legend(title='Metric', fontsize=12)
    
    # Add text labels on bars
    for i, bar in enumerate(grouped_bar.patches):
        height = bar.get_height()
        plt.text(
            bar.get_x() + bar.get_width()/2.,
            height + 1,
            f'{height:.1f}%',
            ha='center', 
            fontsize=10
        )
    
    # Add a note about interpretation
    plt.figtext(0.5, 0.01, 
                "Note: For MSE and MAE, lower values indicate better performance.\n"
                "For R², higher values indicate better performance.", 
                ha="center", fontsize=12, 
                bbox={"facecolor":"lightgray", "alpha":0.5, "pad":5})
        
    plt.tight_layout()
    plt.savefig('model_comparison_grouped_bar.png', dpi=300, bbox_inches='tight')
    plt.show()

if __name__ == "__main__":
    # Get model results
    results = load_model_results()
    
    # Create bar graph visualizations
    create_bar_graph_comparison(results)
    
    print("\nVisualization complete! Check the saved PNG files.")
